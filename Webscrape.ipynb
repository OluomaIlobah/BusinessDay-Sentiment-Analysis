{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dff620e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d70d03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION_DIR = os.path.join(os.getcwd(), \"chrome_session\")\n",
    "os.makedirs(SESSION_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f946fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_FILE = \"scraper_checkpoint_full.json\"\n",
    "CSV_FILE = \"businessday_full_articles.csv\"\n",
    "MAX_PAGES = 819"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08b4ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(f\"--user-data-dir={SESSION_DIR}\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-infobars\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36\")\n",
    "    driver = uc.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fb57b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sleep(min_sec=2, max_sec=5):\n",
    "    time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\"page\": 1, \"visited_urls\": []}\n",
    "\n",
    "def save_checkpoint(page, visited_urls):\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({\"page\": page, \"visited_urls\": visited_urls}, f)\n",
    "    print(f\"Checkpoint saved: Page {page} with {len(visited_urls)} articles visited.\")\n",
    "\n",
    "\n",
    "def load_existing_data():\n",
    "    if os.path.exists(CSV_FILE):\n",
    "        try:\n",
    "            df = pd.read_csv(CSV_FILE, encoding='utf-8-sig', usecols=[\"Title\", \"Author\", \"Date\", \"Content\", \"URL\"])\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(CSV_FILE, encoding='latin1', usecols=[\"Title\", \"Author\", \"Date\", \"Content\", \"URL\"])\n",
    "        return df.values.tolist(), set(df[\"URL\"].tolist())\n",
    "    return [], set()\n",
    "\n",
    "\n",
    "def scrape_listing_page(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.news\"))\n",
    "        )\n",
    "        posts = driver.find_elements(By.CSS_SELECTOR, \"div.post-info h2.post-title a\")\n",
    "        links = [a.get_attribute(\"href\") for a in posts if a.get_attribute(\"href\")]\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping listing page: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3bd0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(driver, url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"article\"))\n",
    "        )\n",
    "        title = driver.find_element(By.CSS_SELECTOR, \"h1.post-title\").text\n",
    "        author = driver.find_element(By.CSS_SELECTOR, \"p.author-name\").text\n",
    "        date = driver.find_element(By.CSS_SELECTOR, \"p.post-date\").text\n",
    "        content_elem = driver.find_element(By.CSS_SELECTOR, \"article div.post-content\")\n",
    "        content = content_elem.text.strip().replace('\\n', ' ')\n",
    "        return [title, author, date, content, url]\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping article at {url}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def save_data(data):\n",
    "    df = pd.DataFrame(data, columns=['Title', 'Author', 'Date', 'Content', 'URL'])\n",
    "    df.to_csv(CSV_FILE, index=False)\n",
    "    print(f\"Saved {len(data)} articles to {CSV_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722a470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Starting full article scraper for BusinessDay...\")\n",
    "    driver = setup_driver()\n",
    "    checkpoint = load_checkpoint()\n",
    "    scraped_data, visited_urls = load_existing_data()\n",
    "    visited_urls = set(visited_urls)\n",
    "\n",
    "    try:\n",
    "        for page in range(checkpoint['page'], MAX_PAGES + 1):\n",
    "            url = f\"https://businessday.ng/tag/bdlead/page/{page}/?amp\"\n",
    "            print(f\"\\nScraping listing page {page}: {url}\")\n",
    "            driver.get(url)\n",
    "            random_sleep(3, 6)\n",
    "\n",
    "            article_links = scrape_listing_page(driver)\n",
    "            print(f\"Found {len(article_links)} article links\")\n",
    "\n",
    "            for article_url in article_links:\n",
    "                if article_url in visited_urls:\n",
    "                    print(f\"Skipping already visited article: {article_url}\")\n",
    "                    continue\n",
    "\n",
    "                result = scrape_article(driver, article_url)\n",
    "                if result:\n",
    "                    scraped_data.append(result)\n",
    "                    visited_urls.add(article_url)\n",
    "                    print(f\"Scraped: {result[0][:60]}...\")\n",
    "                    random_sleep(2, 4)\n",
    "                else:\n",
    "                    print(f\"Skipped article due to error: {article_url}\")\n",
    "\n",
    "            save_data(scraped_data)\n",
    "            save_checkpoint(page, list(visited_urls))\n",
    "            random_sleep(5, 8)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by user. Saving checkpoint...\")\n",
    "        save_data(scraped_data)\n",
    "        save_checkpoint(page, list(visited_urls))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        save_data(scraped_data)\n",
    "        save_checkpoint(page, list(visited_urls))\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"Scraping complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
